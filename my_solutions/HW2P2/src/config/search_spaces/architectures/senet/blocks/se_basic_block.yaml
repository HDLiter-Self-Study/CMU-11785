# SENet Basic Block with SE module configuration
# @package blocks.se_basic

name: "SEBasicBlock"
description: "Basic SE block with squeeze-and-excitation attention"

# SE Basic block specific parameters
params:
  activation:
    param_name: "se_basic_activation"
    type: "categorical"
    choices: ["relu", "gelu", "swish", "leaky_relu", "elu"]
    description: "Activation function for SE basic blocks"
    
  normalization:
    param_name: "se_basic_normalization"
    type: "categorical"
    choices: ["batch_norm", "group_norm", "layer_norm"]
    description: "Normalization method for SE basic blocks"
    
  dropout_rate:
    param_name: "se_basic_dropout_rate"
    type: "float"
    min: 0.0
    max: 0.3
    description: "Dropout rate in SE basic blocks"
    
  # SE specific parameters
  se_ratio:
    param_name: "se_basic_ratio"
    type: "int"
    min: 4
    max: 32
    description: "SE reduction ratio for basic blocks"
    
  se_activation:
    param_name: "se_basic_se_activation"
    type: "categorical"
    choices: ["relu", "gelu", "swish", "sigmoid"]
    description: "Activation function in SE module"
    
  # Advanced SE options
  use_attention_dropout:
    param_name: "se_basic_attention_dropout"
    type: "categorical"
    choices: [true, false]
    description: "Whether to use dropout in SE attention"
    
  attention_dropout_rate:
    param_name: "se_basic_attention_dropout_rate"
    type: "float"
    min: 0.0
    max: 0.2
    description: "Dropout rate in SE attention"
    condition: "se_basic_attention_dropout == true"

# Block-specific parameter estimation
parameter_estimation:
  base_params: 38000
  se_params_ratio: 0.06
