"""
SEæ¨¡å—æŒ–ç©ºç¤ºä¾‹ - ä¾›å­¦ç”Ÿå¡«å†™ç»ƒä¹ 
è¯·å¡«å†™æ‰€æœ‰æ ‡è®°ä¸º # TODO çš„éƒ¨åˆ†
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class SEModule(nn.Module):
    """
    Squeeze-and-Excitation æ¨¡å—

    è®ºæ–‡: "Squeeze-and-Excitation Networks" (https://arxiv.org/abs/1709.01507)

    æ ¸å¿ƒæ€æƒ³:
    1. Squeeze: é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–åŽ‹ç¼©ç©ºé—´ä¿¡æ¯
    2. Excitation: é€šè¿‡å…¨è¿žæŽ¥å±‚å­¦ä¹ é€šé“é—´çš„ç›¸äº’ä¾èµ–å…³ç³»
    3. Scale: å°†å­¦ä¹ åˆ°çš„æƒé‡åº”ç”¨åˆ°åŽŸå§‹ç‰¹å¾å›¾ä¸Š
    """

    def __init__(self, channels: int, reduction: int = 16):
        """
        åˆå§‹åŒ–SEæ¨¡å—

        Args:
            channels: è¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°
            reduction: é€šé“é™ç»´æ¯”ä¾‹ï¼Œç”¨äºŽå‡å°‘å‚æ•°é‡
        """
        super().__init__()

        # TODO: å®žçŽ°å…¨å±€å¹³å‡æ± åŒ–
        # æç¤º: ä½¿ç”¨ nn.AdaptiveAvgPool2d å°†ç©ºé—´ç»´åº¦åŽ‹ç¼©ä¸º 1x1
        # ç›®æ ‡: [B, C, H, W] -> [B, C, 1, 1]
        self.global_avgpool = None

        # TODO: å®žçŽ°ç¬¬ä¸€ä¸ªå…¨è¿žæŽ¥å±‚(é€šé“é™ç»´)
        # æç¤º: è¾“å…¥é€šé“=channels, è¾“å‡ºé€šé“=channels//reduction
        # æ³¨æ„: å¯ä»¥ä½¿ç”¨ nn.Linear æˆ– nn.Conv2d(kernel_size=1)
        self.fc1 = None

        # TODO: å®žçŽ°ReLUæ¿€æ´»å‡½æ•°
        # æç¤º: ä½¿ç”¨ nn.ReLU(inplace=True) èŠ‚çœå†…å­˜
        self.relu = None

        # TODO: å®žçŽ°ç¬¬äºŒä¸ªå…¨è¿žæŽ¥å±‚(é€šé“å‡ç»´)
        # æç¤º: è¾“å…¥é€šé“=channels//reduction, è¾“å‡ºé€šé“=channels
        self.fc2 = None

        # TODO: å®žçŽ°Sigmoidæ¿€æ´»å‡½æ•°
        # æç¤º: ä½¿ç”¨ nn.Sigmoid() ç”Ÿæˆ [0,1] èŒƒå›´çš„æƒé‡
        self.sigmoid = None

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥ç‰¹å¾å›¾ [batch_size, channels, height, width]

        Returns:
            output: ç»è¿‡SEæ¨¡å—è°ƒåˆ¶çš„ç‰¹å¾å›¾ï¼Œå½¢çŠ¶ä¸Žè¾“å…¥ç›¸åŒ
        """
        # èŽ·å–è¾“å…¥çš„å½¢çŠ¶ä¿¡æ¯
        batch_size, channels, height, width = x.size()

        # TODO: å®žçŽ° Squeeze æ“ä½œ
        # æç¤º: ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–åŽ‹ç¼©ç©ºé—´ç»´åº¦
        # ç›®æ ‡: [B, C, H, W] -> [B, C, 1, 1]
        y = None

        # TODO: é‡å¡‘å¼ é‡ä¸º2Dï¼Œä¾¿äºŽå…¨è¿žæŽ¥å±‚å¤„ç†
        # æç¤º: ä½¿ç”¨ .view() æˆ– .reshape()
        # ç›®æ ‡: [B, C, 1, 1] -> [B, C]
        y = None

        # TODO: å®žçŽ° Excitation æ“ä½œ - ç¬¬ä¸€ä¸ªå…¨è¿žæŽ¥å±‚
        # æç¤º: é€šè¿‡fc1é™ç»´
        y = None

        # TODO: åº”ç”¨ReLUæ¿€æ´»
        y = None

        # TODO: å®žçŽ° Excitation æ“ä½œ - ç¬¬äºŒä¸ªå…¨è¿žæŽ¥å±‚
        # æç¤º: é€šè¿‡fc2å‡ç»´å›žåŽŸå§‹é€šé“æ•°
        y = None

        # TODO: åº”ç”¨Sigmoidæ¿€æ´»ï¼Œç”Ÿæˆé€šé“æ³¨æ„åŠ›æƒé‡
        # æ³¨æ„: Sigmoidè¾“å‡ºèŒƒå›´ä¸º[0,1]ï¼Œè¡¨ç¤ºæ¯ä¸ªé€šé“çš„é‡è¦æ€§
        y = None

        # TODO: é‡å¡‘æƒé‡å¼ é‡ä¸ºå¯å¹¿æ’­çš„å½¢çŠ¶
        # æç¤º: [B, C] -> [B, C, 1, 1]ï¼Œä¾¿äºŽä¸ŽåŽŸå§‹ç‰¹å¾å›¾ç›¸ä¹˜
        y = None

        # TODO: å®žçŽ° Scale æ“ä½œ
        # æç¤º: å°†æ³¨æ„åŠ›æƒé‡åº”ç”¨åˆ°åŽŸå§‹ç‰¹å¾å›¾
        # è¿™æ˜¯é€å…ƒç´ ç›¸ä¹˜ï¼Œæ¯ä¸ªé€šé“ä¼šè¢«å¯¹åº”çš„æƒé‡ç¼©æ”¾
        return None


# ====================== éªŒè¯ä»£ç  ======================


def test_se_module():
    """æµ‹è¯•SEæ¨¡å—çš„å®žçŽ°æ­£ç¡®æ€§"""
    print("ðŸ§ª å¼€å§‹æµ‹è¯• SEModule...")

    # æµ‹è¯•ç”¨ä¾‹1: åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    print("   æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½...")
    channels = 64
    se_module = SEModule(channels, reduction=16)

    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size, height, width = 2, 32, 32
    x = torch.randn(batch_size, channels, height, width)

    # å‰å‘ä¼ æ’­
    output = se_module(x)

    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shape = (batch_size, channels, height, width)
    assert output.shape == expected_shape, f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ›{expected_shape}, å®žé™…{output.shape}"
    print("      âœ… è¾“å‡ºå½¢çŠ¶æ­£ç¡®")

    # æµ‹è¯•ç”¨ä¾‹2: å‚æ•°æ•°é‡éªŒè¯
    print("   æµ‹è¯•2: å‚æ•°æ•°é‡...")
    total_params = sum(p.numel() for p in se_module.parameters())
    # SEæ¨¡å—å‚æ•° = fc1æƒé‡ + fc1åç½® + fc2æƒé‡ + fc2åç½®
    # = (channels * channels//reduction) + channels//reduction + (channels//reduction * channels) + channels
    expected_params = channels * (channels // 16) + (channels // 16) + (channels // 16) * channels + channels
    expected_params = channels * (channels // 16) * 2 + (channels // 16) + channels
    print(f"      å‚æ•°æ€»é‡: {total_params}")
    print("      âœ… å‚æ•°æ•°é‡åˆç†")

    # æµ‹è¯•ç”¨ä¾‹3: æ¢¯åº¦åå‘ä¼ æ’­
    print("   æµ‹è¯•3: æ¢¯åº¦åå‘ä¼ æ’­...")
    loss = output.sum()
    loss.backward()

    # æ£€æŸ¥æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦
    for name, param in se_module.named_parameters():
        assert param.grad is not None, f"å‚æ•° {name} æ²¡æœ‰æ¢¯åº¦"
    print("      âœ… æ¢¯åº¦åå‘ä¼ æ’­æ­£å¸¸")

    # æµ‹è¯•ç”¨ä¾‹4: æ³¨æ„åŠ›æƒé‡èŒƒå›´
    print("   æµ‹è¯•4: æ³¨æ„åŠ›æœºåˆ¶...")
    with torch.no_grad():
        # æ‰‹åŠ¨æ£€æŸ¥ä¸­é—´ç»“æžœ
        y = se_module.global_avgpool(x)
        y = y.view(y.size(0), -1)
        y = se_module.fc1(y)
        y = se_module.relu(y)
        y = se_module.fc2(y)
        weights = se_module.sigmoid(y)

        # éªŒè¯Sigmoidè¾“å‡ºèŒƒå›´
        assert torch.all(weights >= 0) and torch.all(weights <= 1), "æ³¨æ„åŠ›æƒé‡ä¸åœ¨[0,1]èŒƒå›´å†…"
        print("      âœ… æ³¨æ„åŠ›æƒé‡èŒƒå›´æ­£ç¡®")

    print("ðŸŽ‰ SEModule æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼\n")


def test_se_integration():
    """æµ‹è¯•SEæ¨¡å—ä¸Žå…¶ä»–å±‚çš„é›†æˆ"""
    print("ðŸ§ª æµ‹è¯• SEæ¨¡å—é›†æˆ...")

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ®‹å·®å—ï¼Œé›†æˆSEæ¨¡å—
    class SimpleResidualWithSE(nn.Module):
        def __init__(self, channels):
            super().__init__()
            self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
            self.bn1 = nn.BatchNorm2d(channels)
            self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
            self.bn2 = nn.BatchNorm2d(channels)
            self.se = SEModule(channels)
            self.relu = nn.ReLU(inplace=True)

        def forward(self, x):
            identity = x
            out = self.relu(self.bn1(self.conv1(x)))
            out = self.bn2(self.conv2(out))
            out = self.se(out)  # åº”ç”¨SEæ¨¡å—
            out += identity  # æ®‹å·®è¿žæŽ¥
            out = self.relu(out)
            return out

    # æµ‹è¯•é›†æˆæ•ˆæžœ
    channels = 64
    block = SimpleResidualWithSE(channels)
    x = torch.randn(2, channels, 32, 32)
    output = block(x)

    assert output.shape == x.shape, "é›†æˆæµ‹è¯•å¤±è´¥"
    print("   âœ… SEæ¨¡å—é›†æˆæµ‹è¯•é€šè¿‡")
    print()


if __name__ == "__main__":
    """
    è¿è¡Œæµ‹è¯•å‰ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®žçŽ° SEModule ä¸­çš„æ‰€æœ‰ TODO é¡¹
    """
    try:
        test_se_module()
        test_se_integration()
        print("ðŸŽŠ æ­å–œï¼SEæ¨¡å—å®žçŽ°å®Œå…¨æ­£ç¡®ï¼")

        # é¢å¤–çš„å­¦ä¹ æ£€æŸ¥
        print("\nðŸ“š å­¦ä¹ æ£€æŸ¥ç‚¹:")
        print("   1. ä½ ç†è§£SEæ¨¡å—çš„ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤äº†å—ï¼Ÿ(Squeeze, Excitation, Scale)")
        print("   2. ä¸ºä»€ä¹ˆè¦ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–è€Œä¸æ˜¯æœ€å¤§æ± åŒ–ï¼Ÿ")
        print("   3. reductionå‚æ•°çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ")
        print("   4. SEæ¨¡å—å¦‚ä½•æ”¹å–„ç½‘ç»œçš„è¡¨å¾èƒ½åŠ›ï¼Ÿ")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("\nðŸ” è°ƒè¯•æç¤º:")
        print("   1. æ£€æŸ¥æ‰€æœ‰ TODO é¡¹æ˜¯å¦éƒ½å·²å®žçŽ°")
        print("   2. ç¡®è®¤å¼ é‡å½¢çŠ¶å˜æ¢æ˜¯å¦æ­£ç¡®")
        print("   3. éªŒè¯æ¯ä¸€æ­¥çš„è¾“å…¥è¾“å‡ºç»´åº¦")
        print("   4. å¯ä»¥æ·»åŠ  print() è¯­å¥æŸ¥çœ‹ä¸­é—´ç»“æžœ")
